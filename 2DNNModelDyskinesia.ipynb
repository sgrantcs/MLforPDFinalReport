{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#import keras\n",
    "\n",
    "import statistics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mode( input_data ):\n",
    "\n",
    "    freq_count = dict() \n",
    "\n",
    "    for d in input_data: \n",
    "        if ( d in freq_count ):\n",
    "            freq_count[d] += 1 \n",
    "        else: \n",
    "            freq_count[d] = 1 \n",
    "\n",
    "    max_count = 0 \n",
    "    mode_value = None \n",
    "\n",
    "    for k,v in freq_count.items():\n",
    "        if ( v > max_count ):\n",
    "            mode_value = k \n",
    "            max_count = v\n",
    "\n",
    "    return mode_value ; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(input_data):\n",
    "\n",
    "    #print(input_data[0:100])\n",
    "    max_value = np.argmax(input_data)\n",
    "    min_value = np.argmin(input_data)\n",
    "    for i in range(0, len(input_data)):\n",
    "        input_data[i] = (input_data[i] - min_value)/(max_value-min_value)\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_samples( input_dataset, x_columns, y_column, sample_size, y_compress_function=my_mode):\n",
    "\n",
    "    y_values = [] \n",
    "\n",
    "    # First take the y series data and apply the compression function to get a single value per sample. \n",
    "    y_series = input_dataset[y_column]; \n",
    "\n",
    "    for i in range(0,len(input_dataset), sample_size): \n",
    "\n",
    "        if ( (i + 128) <= len(y_series)):\n",
    "            value = y_compress_function(y_series[i:i+sample_size])\n",
    "            y_values.append(value)\n",
    "            \n",
    "\n",
    "    # Now truncate the dataset, since we need the rows to be divisible by the sample size. \n",
    "    input_dataset = input_dataset.truncate(after=len(y_values*sample_size)-1)\n",
    "\n",
    "    input_dataset = input_dataset[x_columns]\n",
    "    input_dataset.replace(np.nan, 0)\n",
    "    np_x_values = input_dataset.to_numpy()\n",
    "    np_x_values = np_x_values.astype(np.float32)\n",
    "    \n",
    "    #Regularize the data \n",
    "    np_x_values.shape\n",
    "    for i in range (0, np_x_values.shape[1]): \n",
    "        normalise_data(np_x_values[:,i])     \n",
    "    np_x_values.shape\n",
    "    np_x_values = np_x_values.reshape(-1, sample_size, np_x_values.shape[1] )\n",
    "    np_x_values = np_x_values.reshape(np_x_values.shape[0], np_x_values.shape[1], np_x_values.shape[2], 1) \n",
    "    \n",
    "    np_y_values = np.array(y_values); \n",
    "    np_y_values = np_y_values.reshape(np_y_values.shape[0], 1)\n",
    "\n",
    "    return np_x_values.astype(np.float32), np_y_values.astype(np.int32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svetl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "traindata_day1 = pd.read_csv(\"./3_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "#traindata_day1.head()\n",
    "traindata_day4 = pd.read_csv(\"./3_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "#traindata_day4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Patient 10_BOS data for days 1 and 4 to create a single dataframe that will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.30005 -0.01596 -0.14405 -0.01809 -0.32538 -0.12244 -0.18662 -0.10299\n",
      " -0.03336 -0.15363 -0.20641 -0.28103 -0.17679 -0.31298 -0.09842 -0.20488\n",
      " -0.09749 -0.27295 -0.21574 -0.24924 -0.17514 -0.02196 -0.21193 -0.12727\n",
      " -0.17258 -0.15161 -0.14614 -0.26023 -0.25426  0.01169 -0.13273 -0.14219\n",
      " -0.29519 -0.19437 -0.14852 -0.14671 -0.30756 -0.0338  -0.12935 -0.17719\n",
      "  0.00575 -0.15944 -0.15554 -0.1585  -0.15541 -0.1604  -0.14627  0.01069\n",
      " -0.21119 -0.28041 -0.1633  -0.10771 -0.03111 -0.16023 -0.21065 -0.27878\n",
      " -0.14868 -0.09378 -0.03699 -0.17882 -0.16607 -0.07461 -0.09938 -0.41613\n",
      " -0.0966   0.00106  0.0266  -0.12087 -0.05813 -0.0754  -0.19863 -0.1264\n",
      " -0.19187 -0.01346 -0.11302 -0.18965 -0.00946 -0.11697 -0.31359 -0.17391\n",
      " -0.15066 -0.16317 -0.0042  -0.15053 -0.16506 -0.14127 -0.01272 -0.14971\n",
      "  0.00295 -0.18557 -0.12479  0.00946 -0.01262 -0.22698 -0.27684 -0.17312\n",
      " -0.3261  -0.0423  -0.06494 -0.16015]\n",
      "[-0.15265 -0.16153 -0.15065 -0.1685  -0.13273 -0.02342 -0.34851 -0.27445\n",
      " -0.14706 -0.16416 -0.14864 -0.1687  -0.13517 -0.21369 -0.29142 -0.11491\n",
      " -0.19702 -0.07595  0.00483  0.01469 -0.07783 -0.17311 -0.15481 -0.14423\n",
      " -0.2544  -0.25983 -0.0115  -0.01643  0.03077 -0.13818 -0.0423  -0.08742\n",
      " -0.20131  0.00179 -0.13308 -0.168   -0.14942 -0.16462 -0.14978 -0.31201\n",
      " -0.15989 -0.15468 -0.15813  0.00077 -0.16473  0.0309  -0.04018 -0.11949\n",
      " -0.03835 -0.09759  0.12802 -0.19869 -0.1571  -0.09943 -0.06619 -0.34306\n",
      " -0.04928 -0.00671 -0.01983 -0.27212 -0.24016 -0.15884 -0.08028  0.00955\n",
      "  0.02028 -0.10856 -0.06894 -0.06559 -0.20413 -0.04483  0.03933 -0.14285\n",
      " -0.12118 -0.42998 -0.24225 -0.12617 -0.05309  0.0412  -0.16397 -0.01124\n",
      "  0.00312 -0.00089 -0.15336 -0.31775 -0.13992 -0.00519  0.01312 -0.02807\n",
      " -0.13422 -0.01657 -0.15417 -0.19991 -0.26713  0.01585 -0.19922 -0.12643\n",
      " -0.20669 -0.3182  -0.0365  -0.09462]\n",
      "[-10.0837  -10.39925 -10.16389 -10.05944 -10.20841 -10.1756  -10.03628\n",
      " -10.03466 -10.07171 -10.19437  -9.99621 -10.09565 -10.22228 -10.41693\n",
      " -10.2053  -10.08818 -10.14243 -10.11246 -10.44137 -10.23934 -10.19319\n",
      " -10.1899  -10.19328 -10.19452 -10.18827 -10.20079 -10.18714 -10.13732\n",
      "  -9.8645  -10.1242  -10.09901 -10.11852 -10.24363 -10.14842 -10.37074\n",
      " -10.07304 -10.16662 -10.21629 -10.18614 -10.20941 -10.03285  -9.89387\n",
      " -10.18741 -10.20913 -10.17799 -10.06677 -10.3573  -10.17731 -10.20552\n",
      " -10.19492 -10.19703 -10.20576 -10.16516 -10.03183 -10.2951  -10.28755\n",
      " -10.23382 -10.37909 -10.31464 -10.08156 -10.12293 -10.11833 -10.14106\n",
      " -10.43171 -10.23033 -10.1338   -9.95803 -10.20389 -10.03162 -10.28406\n",
      " -10.14344 -10.12229 -10.10849 -10.12326 -10.12996  -9.85476 -10.15066\n",
      " -10.24124  -9.89591 -10.15656 -10.21693 -10.03536 -10.20141 -10.19362\n",
      " -10.04808 -10.1967  -10.20458 -10.18416 -10.23409 -10.33178 -10.20626\n",
      " -10.14828 -10.09589 -10.35949 -10.17174 -10.19373 -10.24911 -10.29412\n",
      "  -9.95645 -10.17639]\n",
      "4937348\n"
     ]
    }
   ],
   "source": [
    "trainingdata = pd.concat([traindata_day1, traindata_day4], ignore_index=True)\n",
    "X_train, Y_train = generate_data_samples(trainingdata, ['leftAnkle_X', 'leftAnkle_Y','leftAnkle_Z'], 'Dysk_Score', 128)\n",
    "#print(X_train[0:10])\n",
    "#print(Y_train[0:10])\n",
    "print(len(trainingdata))\n",
    "#training_data = trainingdata[ ['leftAnkle_X', 'leftAnkle_Y','leftAnkle_Z'] ] #create a dataframe for 3 axel readings\n",
    "#training_labels = trainingdata['Dysk_Score'] # create a dataframe for the labels\n",
    "#print(len(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-2921a55935f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m class_weights = class_weight.compute_class_weight('balanced',\n\u001b[0;32m      4\u001b[0m                                                  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                                                  Y_train)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m                           \"will result in an error\", FutureWarning)\n\u001b[0;32m     73\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\class_weight.py\u001b[0m in \u001b[0;36mcompute_class_weight\u001b[1;34m(class_weight, classes, y)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         raise ValueError(\"classes should include all valid labels that can \"\n\u001b[0;32m     45\u001b[0m                          \"be in y\")\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "print(np.unique(Y_train))\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(Y_train),\n",
    "                                                 Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>leftAnkle_X</th>\n",
       "      <th>leftAnkle_Y</th>\n",
       "      <th>leftAnkle_Z</th>\n",
       "      <th>leftAnkle_Magnitude</th>\n",
       "      <th>Dysk_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.444037e+09</td>\n",
       "      <td>0.15686</td>\n",
       "      <td>0</td>\n",
       "      <td>9.72549</td>\n",
       "      <td>9.72675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.444037e+09</td>\n",
       "      <td>0.31372</td>\n",
       "      <td>0</td>\n",
       "      <td>9.56863</td>\n",
       "      <td>9.57377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.444037e+09</td>\n",
       "      <td>0.15686</td>\n",
       "      <td>-0.15686</td>\n",
       "      <td>9.88235</td>\n",
       "      <td>9.88484</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.444037e+09</td>\n",
       "      <td>0.47059</td>\n",
       "      <td>-0.15686</td>\n",
       "      <td>10.0392</td>\n",
       "      <td>10.0515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.444037e+09</td>\n",
       "      <td>0.15686</td>\n",
       "      <td>0.15686</td>\n",
       "      <td>9.56863</td>\n",
       "      <td>9.5712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     timestamp leftAnkle_X leftAnkle_Y leftAnkle_Z  \\\n",
       "0           0  1.444037e+09     0.15686           0     9.72549   \n",
       "1           1  1.444037e+09     0.31372           0     9.56863   \n",
       "2           2  1.444037e+09     0.15686    -0.15686     9.88235   \n",
       "3           3  1.444037e+09     0.47059    -0.15686     10.0392   \n",
       "4           4  1.444037e+09     0.15686     0.15686     9.56863   \n",
       "\n",
       "  leftAnkle_Magnitude  Dysk_Score  \n",
       "0             9.72675           0  \n",
       "1             9.57377           0  \n",
       "2             9.88484           0  \n",
       "3             10.0515           0  \n",
       "4              9.5712           0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valdata_day1 = pd.read_csv(\"./17_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "valdata_day4 = pd.read_csv(\"./17_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "valdata_day1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Patient 17_BOS data for days 1 and 4 to create a single dataframe that will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15686 0.31372 0.15686 0.47059 0.15686 0.47059 0.31372 0.31373 0.31372\n",
      " 0.31372 0.31372 0.31372 0.15686 0.15686 0.15686 0.47059 0.31372 0.15686\n",
      " 0.15686 0.31373 0.31372 0.31373 0.47059 0.15686 0.47059 0.31372 0.15686\n",
      " 0.47059 0.31372 0.15686 0.31372 0.15686 0.31373 0.31372 0.15686 0.31372\n",
      " 0.31372 0.30754 0.15717 0.32099 0.30671 0.15299 0.157   0.16022 0.32485\n",
      " 0.46975 0.30362 0.15107 0.16781 0.48195 0.45892 0.14283 0.17997 0.62299\n",
      " 0.14476 0.32417 0.3064  0.15105 0.16508 0.31133 0.15594 0.32209 0.30355\n",
      " 0.16457 0.47687 0.30096 0.15462 0.15623 0.16166 0.31984 0.30683 0.15364\n",
      " 0.15425 0.17057 0.46768 0.15021 0.32316 0.30499 0.15774 0.32127 0.30504\n",
      " 0.15941 0.31448 0.14901 0.16518 0.31298 0.14924 0.16573 0.31052 0.15854\n",
      " 0.31251 0.15777 0.31357 0.1543  0.32643 0.46935 0.30367 0.15129 0.16687\n",
      " 0.3039 ]\n",
      "[ 0.0000e+00  0.0000e+00 -1.5686e-01 -1.5686e-01  1.5686e-01  0.0000e+00\n",
      "  1.5686e-01 -1.5686e-01  0.0000e+00  0.0000e+00  0.0000e+00  1.5686e-01\n",
      " -3.1372e-01  0.0000e+00  0.0000e+00 -1.5686e-01  1.5686e-01 -1.5686e-01\n",
      "  0.0000e+00  1.5686e-01 -0.0000e+00 -1.5686e-01  1.5686e-01  0.0000e+00\n",
      "  0.0000e+00 -1.5686e-01  0.0000e+00  0.0000e+00  0.0000e+00  1.5686e-01\n",
      "  0.0000e+00  1.5686e-01 -3.1373e-01  0.0000e+00 -1.5686e-01 -1.5686e-01\n",
      "  0.0000e+00 -1.3140e-02 -3.1683e-01 -1.5137e-01 -1.5334e-01  5.0800e-03\n",
      " -1.4900e-03  8.6000e-04 -1.9600e-03  6.9900e-03  1.5551e-01 -3.8600e-03\n",
      " -5.6000e-03 -1.5524e-01  1.3900e-03 -1.6634e-01 -1.4267e-01  1.5890e-01\n",
      " -2.2300e-03  1.6604e-01  1.4472e-01 -1.6681e-01 -1.4967e-01  5.8600e-03\n",
      " -8.2800e-03 -1.5427e-01  1.7000e-04 -1.6242e-01 -1.5716e-01 -1.5009e-01\n",
      " -2.1600e-03 -1.5728e-01  6.1000e-03 -1.6400e-03  4.4000e-04 -1.1000e-04\n",
      " -0.0000e+00  1.2000e-04 -4.9000e-04  1.8200e-03 -6.8200e-03 -1.5606e-01\n",
      "  5.9100e-03 -2.0700e-03  2.3700e-03 -7.4200e-03 -1.5420e-01 -9.5000e-04\n",
      " -1.5801e-01  7.8400e-03 -7.8700e-03 -1.5789e-01  1.4280e-02  1.4777e-01\n",
      " -1.6171e-01  8.4100e-03 -6.4400e-03 -1.6419e-01 -1.4343e-01  1.5980e-01\n",
      " -5.1100e-03 -4.9100e-03 -1.5678e-01  6.8700e-03]\n",
      "[ 9.72549  9.56863  9.88235 10.03922  9.56863  9.56863  9.72549  9.88235\n",
      "  9.72549  9.72549  9.88235  9.56863  9.88235  9.72549  9.72549  9.72549\n",
      "  9.72549  9.72549  9.88235  9.72549  9.56863  9.72549  9.72549  9.88235\n",
      "  9.72549  9.88235  9.72549  9.88235  9.72549  9.72549  9.88235  9.72549\n",
      "  9.72549  9.72549  9.72549  9.72549  9.72549  9.56934  9.73017  9.72845\n",
      "  9.89049  9.86916  9.56394  9.73739  9.8865   9.87624  9.72113  9.72666\n",
      "  9.72517  9.72559  9.72542  9.72567  9.72486  9.72785  9.71667  9.57691\n",
      "  9.88497  9.72682  9.87665  9.56316  9.73297  9.7234   9.72636  9.72411\n",
      "  9.73017  9.88969  9.87065  9.55878  9.575    9.73455  9.8868   9.87785\n",
      "  9.71436  9.57061  9.73099  9.72389  9.7264   9.72346  9.73271  9.88016\n",
      "  9.72477  9.88973  9.87591  9.71919  9.73476  9.87619  9.55704  9.57641\n",
      "  9.73063  9.7195   9.56278  9.57565  9.72791  9.73117  9.88186  9.7195\n",
      "  9.72758  9.72311  9.73292  9.87966]\n",
      "5307298\n",
      "[[[[ 0.02921942]\n",
      "   [ 0.6561393 ]\n",
      "   [ 0.3493499 ]]\n",
      "\n",
      "  [[ 0.35590217]\n",
      "   [ 0.6561393 ]\n",
      "   [ 0.02403669]]\n",
      "\n",
      "  [[ 0.02921942]\n",
      "   [ 0.33129   ]\n",
      "   [ 0.67466503]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.03894536]\n",
      "   [ 0.65373707]\n",
      "   [ 0.33593222]]\n",
      "\n",
      "  [[ 0.34149036]\n",
      "   [ 0.6540269 ]\n",
      "   [ 0.35393453]]\n",
      "\n",
      "  [[-0.3055648 ]\n",
      "   [ 0.66701186]\n",
      "   [ 0.3443934 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.04808815]\n",
      "   [ 1.0063372 ]\n",
      "   [ 0.38166186]]\n",
      "\n",
      "  [[ 0.06135454]\n",
      "   [ 0.94984156]\n",
      "   [ 0.66004485]]\n",
      "\n",
      "  [[ 0.3150408 ]\n",
      "   [ 0.64882886]\n",
      "   [ 0.3572929 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.4396451 ]\n",
      "   [ 0.6932922 ]\n",
      "   [ 0.03206277]]\n",
      "\n",
      "  [[ 0.6374959 ]\n",
      "   [ 0.5904695 ]\n",
      "   [ 0.3841302 ]]\n",
      "\n",
      "  [[-0.0828682 ]\n",
      "   [ 0.39306647]\n",
      "   [ 0.32772022]]]\n",
      "\n",
      "\n",
      " [[[-0.22075975]\n",
      "   [ 1.0049496 ]\n",
      "   [ 0.40217406]]\n",
      "\n",
      "  [[ 0.36771077]\n",
      "   [ 0.5822478 ]\n",
      "   [ 0.6500884 ]]\n",
      "\n",
      "  [[-0.00943435]\n",
      "   [ 0.31157455]\n",
      "   [ 0.3553447 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.28221846]\n",
      "   [ 0.24223912]\n",
      "   [ 0.6911741 ]]\n",
      "\n",
      "  [[ 0.05304483]\n",
      "   [ 0.42121068]\n",
      "   [ 0.28663453]]\n",
      "\n",
      "  [[ 0.40492755]\n",
      "   [ 0.6116139 ]\n",
      "   [ 0.4275794 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 0.17023493]\n",
      "   [ 0.43119267]\n",
      "   [ 0.31172937]]\n",
      "\n",
      "  [[ 0.40798903]\n",
      "   [ 0.4683248 ]\n",
      "   [ 0.3557383 ]]\n",
      "\n",
      "  [[ 0.2924442 ]\n",
      "   [ 0.6983453 ]\n",
      "   [ 0.3614424 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.38570476]\n",
      "   [ 0.4748276 ]\n",
      "   [ 0.35866353]]\n",
      "\n",
      "  [[ 0.38889122]\n",
      "   [ 0.7232174 ]\n",
      "   [ 0.31716448]]\n",
      "\n",
      "  [[ 0.13987003]\n",
      "   [ 0.485493  ]\n",
      "   [ 0.5229172 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.18795818]\n",
      "   [ 0.22403549]\n",
      "   [ 0.7364053 ]]\n",
      "\n",
      "  [[ 0.3668777 ]\n",
      "   [ 0.74376124]\n",
      "   [ 0.49574766]]\n",
      "\n",
      "  [[ 0.5045818 ]\n",
      "   [ 0.82974714]\n",
      "   [ 0.27956375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.18531324]\n",
      "   [ 0.86484975]\n",
      "   [ 0.12078473]]\n",
      "\n",
      "  [[-0.09344801]\n",
      "   [ 0.84235924]\n",
      "   [ 0.26114613]]\n",
      "\n",
      "  [[ 0.07861964]\n",
      "   [ 0.35690767]\n",
      "   [ 0.09676781]]]\n",
      "\n",
      "\n",
      " [[[ 0.41896453]\n",
      "   [ 0.57831305]\n",
      "   [ 0.41928235]]\n",
      "\n",
      "  [[ 0.35298648]\n",
      "   [ 0.4339263 ]\n",
      "   [ 0.770978  ]]\n",
      "\n",
      "  [[ 0.30460683]\n",
      "   [ 0.5064717 ]\n",
      "   [ 0.42762095]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.4063854 ]\n",
      "   [ 0.504815  ]\n",
      "   [ 0.5122567 ]]\n",
      "\n",
      "  [[ 0.08045234]\n",
      "   [ 0.9540456 ]\n",
      "   [ 0.48496446]]\n",
      "\n",
      "  [[ 0.25052065]\n",
      "   [ 0.75736743]\n",
      "   [ 0.31484053]]]]\n"
     ]
    }
   ],
   "source": [
    "valdata = pd.concat([valdata_day1, valdata_day4], ignore_index=True)\n",
    "X_val, Y_val = generate_data_samples(valdata, ['leftAnkle_X', 'leftAnkle_Y','leftAnkle_Z'], 'Dysk_Score', 128)\n",
    "#print(X)\n",
    "#print(Y)\n",
    "\n",
    "print(len(valdata))\n",
    "print(X_val[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>leftAnkle_X</th>\n",
       "      <th>leftAnkle_Y</th>\n",
       "      <th>leftAnkle_Z</th>\n",
       "      <th>leftAnkle_Magnitude</th>\n",
       "      <th>Dysk_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.430815e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.47059</td>\n",
       "      <td>10.1961</td>\n",
       "      <td>10.2069</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.430815e+09</td>\n",
       "      <td>-0.15686</td>\n",
       "      <td>-0.15686</td>\n",
       "      <td>10.1961</td>\n",
       "      <td>10.1985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.430815e+09</td>\n",
       "      <td>-0</td>\n",
       "      <td>-0.31372</td>\n",
       "      <td>10.1961</td>\n",
       "      <td>10.2009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.430815e+09</td>\n",
       "      <td>-0.15686</td>\n",
       "      <td>-0.15686</td>\n",
       "      <td>10.1961</td>\n",
       "      <td>10.1985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.430815e+09</td>\n",
       "      <td>-0.31372</td>\n",
       "      <td>-0.15686</td>\n",
       "      <td>10.1961</td>\n",
       "      <td>10.2021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     timestamp leftAnkle_X leftAnkle_Y leftAnkle_Z  \\\n",
       "0           0  1.430815e+09           0    -0.47059     10.1961   \n",
       "1           1  1.430815e+09    -0.15686    -0.15686     10.1961   \n",
       "2           2  1.430815e+09          -0    -0.31372     10.1961   \n",
       "3           3  1.430815e+09    -0.15686    -0.15686     10.1961   \n",
       "4           4  1.430815e+09    -0.31372    -0.15686     10.1961   \n",
       "\n",
       "  leftAnkle_Magnitude  Dysk_Score  \n",
       "0             10.2069           0  \n",
       "1             10.1985           0  \n",
       "2             10.2009           0  \n",
       "3             10.1985           0  \n",
       "4             10.2021           0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata_day1 = pd.read_csv(\"./11_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "testdata_day4 = pd.read_csv(\"./11_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "testdata_day1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0000e+00 -1.5686e-01 -0.0000e+00 -1.5686e-01 -3.1372e-01 -3.1373e-01\n",
      " -1.5687e-01 -1.5686e-01 -1.5686e-01 -1.5686e-01 -1.5686e-01 -3.1372e-01\n",
      " -1.5688e-01 -1.0000e-05  2.0000e-05 -3.1373e-01 -1.0000e-05 -1.5685e-01\n",
      " -1.5686e-01 -3.1372e-01 -1.5688e-01 -1.5686e-01 -1.5685e-01 -3.1372e-01\n",
      " -1.5690e-01  3.0000e-05 -3.1373e-01 -0.0000e+00 -3.1370e-01 -1.5689e-01\n",
      " -1.5684e-01 -3.1372e-01 -1.5690e-01 -2.8000e-04 -1.6851e-01 -3.1371e-01\n",
      " -1.4299e-01 -8.6800e-03 -3.1971e-01 -1.4424e-01  2.3600e-03  2.4000e-04\n",
      " -3.3500e-03 -1.6828e-01 -3.1153e-01 -1.5197e-01 -1.5637e-01 -1.6374e-01\n",
      " -3.1122e-01 -1.5775e-01 -3.1493e-01 -1.4890e-01 -1.6529e-01 -3.1249e-01\n",
      " -1.5111e-01 -1.5885e-01 -1.5465e-01 -1.6375e-01 -3.1289e-01 -1.5104e-01\n",
      " -1.5875e-01 -1.5512e-01 -1.6196e-01 -3.1956e-01 -3.0749e-01 -1.5147e-01\n",
      " -1.6246e-01 -3.2121e-01 -3.0037e-01  2.9900e-03 -1.6222e-01 -1.6064e-01\n",
      " -3.1773e-01 -3.1614e-01 -3.0003e-01 -3.4600e-03 -3.1806e-01 -1.5604e-01\n",
      " -3.1493e-01 -1.5060e-01 -1.5853e-01 -1.5645e-01 -1.5685e-01 -1.5733e-01\n",
      " -1.5502e-01 -1.6379e-01 -3.1229e-01 -1.5343e-01 -1.4983e-01 -7.1600e-03\n",
      " -3.1877e-01 -1.4945e-01 -1.5933e-01 -1.5443e-01 -1.6415e-01 -3.1139e-01\n",
      " -1.5667e-01 -3.1906e-01 -3.1472e-01 -3.0437e-01]\n",
      "[-4.7059e-01 -1.5686e-01 -3.1372e-01 -1.5686e-01 -1.5686e-01 -1.5686e-01\n",
      " -1.5686e-01 -3.1373e-01 -1.5687e-01 -1.5686e-01 -1.5686e-01 -1.5686e-01\n",
      " -1.5687e-01 -1.0000e-05  1.0000e-05 -1.5686e-01 -1.5686e-01 -3.1372e-01\n",
      " -1.5689e-01  0.0000e+00 -1.5685e-01 -1.5688e-01 -1.0000e-05  1.0000e-05\n",
      " -1.5685e-01 -1.5687e-01 -1.5686e-01 -1.5688e-01  0.0000e+00 -1.5686e-01\n",
      " -2.0000e-05  2.0000e-05 -1.5684e-01 -1.4846e-01 -3.5200e-03 -1.5344e-01\n",
      " -7.9200e-03 -3.1226e-01  6.6000e-03 -1.6470e-01 -1.5440e-01 -1.5889e-01\n",
      " -1.5122e-01  3.9800e-03  7.1000e-04 -6.8300e-03 -1.5479e-01  8.0000e-04\n",
      " -1.6440e-01 -1.4978e-01  3.7100e-03  3.2000e-04 -5.0300e-03 -1.6159e-01\n",
      " -1.5517e-01 -1.5892e-01 -1.5031e-01  3.4000e-04 -1.6707e-01 -3.1997e-01\n",
      " -3.0074e-01  3.3000e-03 -1.6307e-01 -1.5757e-01 -1.4781e-01 -1.1070e-02\n",
      " -3.0521e-01  1.6284e-01 -1.7157e-01 -1.4840e-01  5.3300e-03 -7.5800e-03\n",
      " -1.5630e-01  7.6000e-03 -8.7600e-03 -1.5385e-01 -1.0500e-03 -1.5792e-01\n",
      "  7.5500e-03 -6.9400e-03 -1.6107e-01 -1.5526e-01 -1.5906e-01 -1.4967e-01\n",
      " -2.1600e-03 -1.5763e-01  7.5000e-03 -7.0600e-03 -1.6055e-01 -1.5723e-01\n",
      " -1.5169e-01  4.0800e-03  6.7000e-04 -6.7700e-03 -1.5484e-01  9.0000e-04\n",
      " -1.6472e-01 -1.4847e-01 -1.3500e-03 -1.6210e-01]\n",
      "[10.19608 10.19608 10.19608 10.19608 10.19608 10.03922 10.03921 10.19608\n",
      " 10.03922 10.19607 10.19608 10.03922 10.03921 10.19607 10.19608 10.19609\n",
      " 10.03922 10.03921 10.03922 10.03921 10.19606 10.19609 10.03923 10.0392\n",
      " 10.19607 10.19608 10.19608 10.19606 10.35294 10.19611 10.03925  9.88233\n",
      " 10.19606 10.03025 10.04659 10.20013 10.19478 10.19722 10.19282 10.20801\n",
      " 10.51442 10.34254 10.18621 10.04228 10.19594 10.03442 10.03626 10.05588\n",
      " 10.5197  10.33894 10.19539 10.19054 10.03754 10.21059 10.3433  10.03651\n",
      " 10.19655 10.03777 10.20365 10.18947 10.03359 10.04611 10.19864 10.2012\n",
      " 10.35435 10.18302 10.04335 10.19484 10.03779 10.20528 10.18288  9.87702\n",
      " 10.05381 10.1897  10.0479  10.35125 10.03278 10.20356 10.1948  10.1937\n",
      " 10.02556  9.89276 10.19476  9.87274 10.05908 10.35412 10.19142 10.19133\n",
      " 10.03845 10.20614 10.17873  9.8929  10.35698 10.0302  10.20819 10.17876\n",
      "  9.89076 10.36552 10.17939 10.04687]\n",
      "5086077\n",
      "[[[[ 0.99077725]\n",
      "   [ 0.        ]\n",
      "   [ 0.49978405]]\n",
      "\n",
      "  [[ 0.5069402 ]\n",
      "   [ 0.49528757]\n",
      "   [ 0.49978405]]\n",
      "\n",
      "  [[ 0.99077725]\n",
      "   [ 0.24765168]\n",
      "   [ 0.49978405]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.54161006]\n",
      "   [ 0.50014997]\n",
      "   [ 0.28930685]]\n",
      "\n",
      "  [[ 1.0169957 ]\n",
      "   [ 0.76390445]\n",
      "   [ 0.7309884 ]]\n",
      "\n",
      "  [[ 0.9814929 ]\n",
      "   [ 0.7206005 ]\n",
      "   [ 0.24332276]]]\n",
      "\n",
      "\n",
      " [[[ 1.0017273 ]\n",
      "   [ 0.4830052 ]\n",
      "   [ 0.51537985]]\n",
      "\n",
      "  [[ 0.955768  ]\n",
      "   [ 0.50005525]\n",
      "   [ 0.51689523]]\n",
      "\n",
      "  [[ 0.4784701 ]\n",
      "   [ 0.48849913]\n",
      "   [ 0.7368464 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.9634176 ]\n",
      "   [ 0.70640796]\n",
      "   [ 0.30029762]]\n",
      "\n",
      "  [[ 0.4851943 ]\n",
      "   [ 0.4727594 ]\n",
      "   [ 0.74568796]]\n",
      "\n",
      "  [[ 0.43433064]\n",
      "   [ 0.5278878 ]\n",
      "   [ 0.5069098 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.09373841]\n",
      "   [ 0.7602103 ]\n",
      "   [ 0.739227  ]]\n",
      "\n",
      "  [[ 1.0847007 ]\n",
      "   [ 0.7359613 ]\n",
      "   [ 0.4749752 ]]\n",
      "\n",
      "  [[ 0.91338676]\n",
      "   [ 0.7534692 ]\n",
      "   [ 0.5091813 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.41563848]\n",
      "   [ 0.67282885]\n",
      "   [ 0.25901145]]\n",
      "\n",
      "  [[-0.03433068]\n",
      "   [ 0.4984134 ]\n",
      "   [ 0.86000615]]\n",
      "\n",
      "  [[ 0.11360274]\n",
      "   [ 0.43105   ]\n",
      "   [ 0.9116478 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 0.84574336]\n",
      "   [ 0.5898205 ]\n",
      "   [ 0.04309487]]\n",
      "\n",
      "  [[ 0.21764345]\n",
      "   [ 0.45160472]\n",
      "   [-0.44245395]]\n",
      "\n",
      "  [[-0.05342383]\n",
      "   [ 0.6373869 ]\n",
      "   [ 1.4398257 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.8790253 ]\n",
      "   [ 0.76193106]\n",
      "   [ 0.52100646]]\n",
      "\n",
      "  [[ 0.75009257]\n",
      "   [ 0.43479154]\n",
      "   [ 0.4958571 ]]\n",
      "\n",
      "  [[ 0.47128314]\n",
      "   [ 0.5174368 ]\n",
      "   [ 0.49425033]]]\n",
      "\n",
      "\n",
      " [[[ 0.45215914]\n",
      "   [ 0.4671392 ]\n",
      "   [ 0.5258444 ]]\n",
      "\n",
      "  [[ 0.8397903 ]\n",
      "   [ 0.62564135]\n",
      "   [ 0.36195415]]\n",
      "\n",
      "  [[ 0.700401  ]\n",
      "   [ 0.79438925]\n",
      "   [ 0.24919699]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.8174275 ]\n",
      "   [ 0.5116903 ]\n",
      "   [ 0.13911234]]\n",
      "\n",
      "  [[ 0.7953423 ]\n",
      "   [ 0.48006883]\n",
      "   [ 0.3281652 ]]\n",
      "\n",
      "  [[ 0.0332511 ]\n",
      "   [ 0.5397281 ]\n",
      "   [ 0.3601764 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.631678  ]\n",
      "   [ 0.26667506]\n",
      "   [ 0.38979226]]\n",
      "\n",
      "  [[ 1.1237508 ]\n",
      "   [ 0.5957564 ]\n",
      "   [ 0.5183015 ]]\n",
      "\n",
      "  [[ 0.64518815]\n",
      "   [ 0.58457917]\n",
      "   [ 0.5247462 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.95937693]\n",
      "   [ 1.0580174 ]\n",
      "   [ 0.40181783]]\n",
      "\n",
      "  [[ 0.65798885]\n",
      "   [ 0.96912056]\n",
      "   [ 0.5754149 ]]\n",
      "\n",
      "  [[ 0.47449103]\n",
      "   [ 0.48178965]\n",
      "   [ 0.12679341]]]]\n"
     ]
    }
   ],
   "source": [
    "testdata = pd.concat([testdata_day1, testdata_day4], ignore_index=True)\n",
    "X_test, Y_test = generate_data_samples(testdata, ['leftAnkle_X', 'leftAnkle_Y','leftAnkle_Z'], 'Dysk_Score', 128)\n",
    "#print(X)\n",
    "#print(Y)\n",
    "print(len(testdata))\n",
    "print(X_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testdata.shape\n",
    "print(X_test[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam, schedules, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From TinyML book: Very simple model to prototype\n",
    "This is a sequential model, meaning the output of each layer is passed directly into the next one.\n",
    "The first layer is Conv2D: This is a convolutional layer; it directly receives the network’s input, which is a sequence of raw accelerometer data. The input’s shape is provided in the input_shape argument. It’s set to (seq_length, 3, 1), where seq_length is the total number of accelerometer measurements that are passed in (128 by default). Each measurement is composed of three values, representing the x-, y-, and z-axes. \n",
    "The role of the convolutional layer is to take the raw data and extract some basic features that can be interpreted by subsequent layers. The arguments to the Conv2D() function determine how many features will be extracted.\n",
    "The first argument determines how many filters the layer will have. During training, each filter learns to identify a particular feature in the raw data—for example, one filter might learn to identify the signs of an upward motion. For each filter, the layer outputs a feature map that shows where the feature it has learned occurs within the input.\n",
    "The layer defined in the code has 8 filters, meaning that it will learn to recognize and output eight different types of high-level features from the input data. This is reflected in the output shape, (batch_size, 128, 3, 8), which has eight feature channels in its final dimension, one for each feature. The value in each channel indicates the degree to which a feature was present in that location of the input.\n",
    "The convolutional layers slide a window across the data and decide whether a given feature is present in that window. The second argument to Conv2D() is where we provide the dimensions of this window. In our case, it’s (4, 3). This means that the features for which our filters are hunting span four consecutive accelerometer measurements and all three axes. Because the window spans four measurements, each filter analyzes a small snapshot of time, meaning it can generate features that represent a change in acceleration over time. \n",
    "When used for classification, the goal of a CNN is to transform a big, complex input tensor into a small, simple output. This is what the MaxPool2D layer does by congesting the output of the first convolutional layer into a concentrated, high-level representation of the most relevant features it contains. Even though the original input has 3 accelerometer axes for each measurement, a combination of Conv2D and MaxPool2D merges these together into a single value.\n",
    "The Dropout layer randomly sets some of a tensor’s values to zero during training. In this case, by calling Dropout(0.1), we set 10% of the values to zero, entirely obliterating that data. Dropout is a regularization technique and process of improving machine learning models so that they are less likely to overfit their training data. By randomly removing some data between one layer and the next, we force the neural network to learn how to cope with unexpected noise and variation.\n",
    "There can be more Conv2D and MaxPooling layers; let us strat with 2.\n",
    "The Flatten layer is used to transform a multidimensional tensor into one with a single dimension. In this case, our (14, 1, 16) tensor is squished down into a single dimension with shape (224).\n",
    "It’s then fed into a Dense layer with 16 neurons, which considersg all data at once, learning the meanings of various combinations of inputs. The output of this Dense layer will be a set of 16 values representing the content of the original input in a highly compressed form.\n",
    "The final task is to shrink these 16 values down into 5 classes. To do this, we first add some more dropout and then a final Dense layer, which has four neurons; one representing each class of gesture. Each of them is connected to all 16 of the outputs from the previous layer. During training, each neuron will learn the combination of previous-layer activations that correspond to the gesture it represents. \n",
    "The layer is configured with a \"softmax\" activation function, which results in the layer’s output being a set of probabilities that sum to 1. This output is what we see in the model’s output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(# input_shape=(batch, 128, 3)\n",
    "        8, (4, 3),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        input_shape=(128, 3, 1)), #output_shape=(batch, 128, 3, 8)\n",
    "    tf.keras.layers.MaxPool2D((3, 3)),  # (batch, 42, 1, 8)\n",
    "    tf.keras.layers.Dropout(0.1),  # (batch, 42, 1, 8)\n",
    "    tf.keras.layers.Conv2D(16, (4, 1), padding=\"same\", activation=\"relu\", input_shape=(42, 1, 16)),\n",
    "    tf.keras.layers.MaxPool2D((3, 1), padding=\"same\"),  # (batch, 14, 1, 16)\n",
    "    tf.keras.layers.Dropout(0.1),  # (batch, 14, 1, 16)\n",
    "    tf.keras.layers.Flatten(),  # (batch, 224)\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),  # (batch, 16)\n",
    "    tf.keras.layers.Dropout(0.1),  # (batch, 16)\n",
    "    tf.keras.layers.Dense(5, activation=\"softmax\")  # (batch, 4)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a classifier with multiple classes. Thus, the fitting includes class weights. In most cases, classes are not perfectly balanced, so this addition takes this into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "Y_classes = np.unique(Y_train)\n",
    "print(Y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1., 1: 48., 2: 137., 3: 543., 4: 7780.,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(Y_train)\n",
    "print(mean)\n",
    "\n",
    "#class_weight = {0: 1.,\n",
    "#                1: 50.,\n",
    "#                2: 2.\n",
    "#                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\svetl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 92s 15ms/step - loss: 1.2250 - accuracy: 0.5711 - val_loss: nan - val_accuracy: 0.8415\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 17s 14ms/step - loss: 0.9864 - accuracy: 0.6415 - val_loss: nan - val_accuracy: 0.9022\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 17s 14ms/step - loss: 0.9152 - accuracy: 0.6881 - val_loss: nan - val_accuracy: 0.8120\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 17s 14ms/step - loss: 0.9298 - accuracy: 0.6834 - val_loss: nan - val_accuracy: 0.8628\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 16s 14ms/step - loss: 0.9117 - accuracy: 0.6781 - val_loss: nan - val_accuracy: 0.8658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ff1e1b53c8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit - train the model\n",
    "model.fit(\n",
    "        X_train, Y_train, epochs=5,\n",
    "        verbose=1, class_weight=class_weight, \n",
    "        #batch_size=128,\n",
    "        validation_data=(X_val, Y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296/1296 [==============================] - 6s 4ms/step - loss: nan - accuracy: 0.8658\n",
      "[0.7329286  0.24515776 0.00422043 0.00630367 0.01138962]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#evaluate val_dataset before testing on test data. \n",
    "model.evaluate(X_val, Y_val)\n",
    "classifications = model.predict(X_val)\n",
    "print(classifications[0])\n",
    "print(Y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1242/1242 [==============================] - 6s 4ms/step - loss: nan - accuracy: 0.9691\n",
      "tf.Tensor(\n",
      "[[38506    28     0     0     0]\n",
      " [  918     0     0     0     0]\n",
      " [  269     0     0     0     0]\n",
      " [   13     0     0     0     0]\n",
      " [    0     0     0     0     0]], shape=(5, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)\n",
    "pred = np.argmax(model.predict(X_test), axis=1)\n",
    "confusion = tf.math.confusion_matrix(labels=tf.constant(Y_test),\n",
    "                                       predictions=tf.constant(pred),\n",
    "                                       num_classes=5)\n",
    "print(confusion)\n",
    "\n",
    "\n",
    "#print(\"Loss {}, Accuracy {}\".format(loss, acc))\n",
    "# Convert the model to the TensorFlow Lite format without quantization\n",
    "#converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "#tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "#converter = tf.lite.TFLiteConverter.from_keras_model('CNNmodel.h5')\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# These options are for converter optimizaitons\n",
    "# Try the converter without them and explore model size and accuracy\n",
    "# Then use them and reconvert the model and explore model\n",
    "# size an accuracy at that point.\n",
    "\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]    # Uncomment this line for Model 2 and Model 3\n",
    "\n",
    "#def representative_data_gen():                          # Uncomment the following 5 lines for Model 3\n",
    "#     for input_value, _ in X_test.take(100):\n",
    "#         yield [input_value]\n",
    "#converter.representative_dataset = representative_data_gen\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "#tflite_models_file = pathlib.Path(\"./tmp/model1.tflite\")\n",
    "tflite_model_file = pathlib.Path(\"./tmp/model1.tflite\")     # Change the filename here for Model2 and Model3!\n",
    "tflite_model_file.write_bytes(tflite_model)\n",
    "# Without any optimizations I got\n",
    "# 20236  (model1.tflite) - the model that was created with 10 epochs\n",
    "# With the .optimizations property set I got\n",
    "# 9728 (model2.tflite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell each time to test your model's accuracy (make sure to change the filename). \n",
    "#Tqdm is a Python library used to display smart progress bars that show the progress of Python code execution.\n",
    "from tqdm import tqdm\n",
    "# Load TFLite model and allocate tensors.\n",
    "tflite_model_file = './tmp/model1.tflite'                 # Change the filename here for Model 2 and 3 with optimisations\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_file)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "print(input_index)\n",
    "print(output_index)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(0,100)):\n",
    "    data = X_test[i]\n",
    "    data = data.reshape(1, data.shape[0], data.shape[1], data.shape[2]) \n",
    "    #print(data.shape)\n",
    "    label = Y_test[i]\n",
    "    interpreter.set_tensor(input_index, data)\n",
    "    interpreter.invoke()\n",
    "    predictions.append(interpreter.get_tensor(output_index))\n",
    "    \n",
    "#    Y_test_labels.append(label.numpy()[0])\n",
    "#    X_test.append(X_test)\n",
    "\n",
    "# For model 1, \n",
    "# For model 2, \n",
    "# For model 3, \n",
    "# Note: since the it/s will depend on the computer on which the model instance is running -- it would vary a bit.\n",
    "\n",
    "score = 0\n",
    "for item in range(0,100):\n",
    "  prediction=np.argmax(predictions[item])\n",
    "  label = Y_test[item]\n",
    "  if prediction==label:\n",
    "    score=score+1\n",
    "\n",
    "print(\"Out of 100 predictions I got \" + str(score) + \" correct\")\n",
    "\n",
    "# Model 1 - 100 Correct\n",
    "# Model 2 - 99 Correct\n",
    "# Model 3 - 99 Correct\n",
    "# Note: training starts from a random intialization the result could be off by 1 or 2 correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
