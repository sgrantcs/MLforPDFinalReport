{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#import keras\n",
    "\n",
    "import statistics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mode( input_data ):\n",
    "\n",
    "    freq_count = dict() \n",
    "\n",
    "    for d in input_data: \n",
    "        if ( d in freq_count ):\n",
    "            freq_count[d] += 1 \n",
    "        else: \n",
    "            freq_count[d] = 1 \n",
    "\n",
    "    max_count = 0 \n",
    "    mode_value = None \n",
    "\n",
    "    for k,v in freq_count.items():\n",
    "        if ( v > max_count ):\n",
    "            mode_value = k \n",
    "            max_count = v\n",
    "\n",
    "    return mode_value ; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_samples( input_dataset, x_columns, y_column, sample_size, y_compress_function=my_mode):\n",
    "\n",
    "    y_values = [] \n",
    "\n",
    "    # First take the y series data and apply the compression function to get a single value per sample. \n",
    "    y_series = input_dataset[y_column]; \n",
    "\n",
    "    for i in range(0,len(input_dataset), sample_size): \n",
    "\n",
    "        if ( (i + 128) <= len(y_series)):\n",
    "            value = y_compress_function(y_series[i:i+sample_size])\n",
    "            y_values.append(value)\n",
    "            \n",
    "\n",
    "    # Now truncate the dataset, since we need the rows to be divisible by the sample size. \n",
    "    input_dataset = input_dataset.truncate(after=len(y_values*sample_size)-1)\n",
    "\n",
    "    np_x_values = input_dataset[x_columns].to_numpy()\n",
    "    np_x_values.shape\n",
    "    # Now regularise the X values data points\n",
    "    \n",
    "    \n",
    "    np_x_values = np_x_values.reshape(-1, sample_size, np_x_values.shape[1] )\n",
    "    np_x_values = np_x_values.reshape(np_x_values.shape[0], np_x_values.shape[1], np_x_values.shape[2], 1) \n",
    "    \n",
    "    np_y_values = np.array(y_values); \n",
    "    np_y_values = np_y_values.reshape(np_y_values.shape[0], 1)\n",
    "\n",
    "    return np_x_values.astype(np.float32), np_y_values.astype(np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svetl\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4937348\n"
     ]
    }
   ],
   "source": [
    "traindata_day_3_1 = pd.read_csv(\"./3_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_3_4 = pd.read_csv(\"./3_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_3 = pd.concat([traindata_day_3_1, traindata_day_3_4], ignore_index=True)\n",
    "print(len(trainingdata_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileList(rootFolder, searchExp = None):\n",
    "\n",
    "    filesFound = [] \n",
    "\n",
    "    for root, subdirs, files in os.walk('./testdata'): \n",
    "        for fileName in files :\n",
    "            if ( (searchExp is None) or re.search(searchExp, fileName) ):\n",
    "                filesFound.append(os.path.join(root,fileName)) \n",
    "    return filesFound "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = getFileList (\"./Training\", searchExp = \"rawData_Day1_l.csv\", \"rawData_Day4_l.csv\")\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"./3_BOS_LeftLowerLimb/rawData_Day1_l.csv\", \"./3_BOS_LeftLowerLimb/rawData_Day4_l.csv\",\n",
    "             \"./4_BOS_LeftLowerLimb/rawData_Day1_l.csv\", \"./4_BOS_LeftLowerLimb/rawData_Day4_l.csv\",\n",
    "             \"./5_BOS_LeftLowerLimb/rawData_Day1_l.csv\", \"./5_BOS_LeftLowerLimb/rawData_Day4_l.csv\", \n",
    "             \"./6_BOS_LeftLowerLimb/rawData_Day1_l.csv\", \"./6_BOS_LeftLowerLimb/rawData_Day4_l.csv\",\n",
    "             \"./7_BOS_LeftLowerLimb/rawData_Day1_l.csv\", \"./7_BOS_LeftLowerLimb/rawData_Day4_l.csv\",\n",
    "             \"./8_BOS_LeftLowerLimb/rawData_Day1_l.csv\", \"./8_BOS_LeftLowerLimb/rawData_Day4_l.csv\", \n",
    "             \"./9_BOS_LeftLowerLimb/rawData_Day1_l.csv\", \"./9_BOS_LeftLowerLimb/rawData_Day4_l.csv\",\n",
    "             \"./10_BOS_LeftLowerLimb/rawData_Day1_l.csv\", \"./10_BOS_LeftLowerLimb/rawData_Day4_l.csv\",\n",
    "             \"./13_BOS_LeftLowerLimb/rawData_Day1_l.csv\", \"./13_BOS_LeftLowerLimb/rawData_Day4_l.csv\",\n",
    "             \"./14_BOS_LeftLowerLimb/rawData_Day1_l.csv\", \"./14_BOS_LeftLowerLimb/rawData_Day4_l.csv\",\n",
    "             \"./15_BOS_LeftLowerLimb/rawData_Day1_l.csv\", \"./15_BOS_LeftLowerLimb/rawData_Day4_l.csv\",\n",
    "             \"./16_BOS_LeftLowerLimb/rawData_Day1_l.csv\", \"./16_BOS_LeftLowerLimb/rawData_Day4_l.csv\",\n",
    "             \"./17_BOS_LeftLowerLimb/rawData_Day1_l.csv\", \"./17_BOS_LeftLowerLimb/rawData_Day4_l.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_day_3_1 = pd.read_csv(\"./3_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_3_4 = pd.read_csv(\"./3_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_3 = pd.concat([traindata_day_3_1, traindata_day_3_4], ignore_index=True)\n",
    "print(len(trainingdata_3))\n",
    "traindata_day_4_1 = pd.read_csv(\"./4_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_4_4 = pd.read_csv(\"./4_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_4 = pd.concat([traindata_day_4_1, traindata_day_4_4], ignore_index=True)\n",
    "print(len(trainingdata_4))\n",
    "traindata_day_5_1 = pd.read_csv(\"./5_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_5_4 = pd.read_csv(\"./5_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_5 = pd.concat([traindata_day_5_1, traindata_day_5_4], ignore_index=True)\n",
    "print(len(trainingdata_5))\n",
    "traindata_day_6_1 = pd.read_csv(\"./6_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_6_4 = pd.read_csv(\"./6_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_6 = pd.concat([traindata_day_6_1, traindata_day_6_4], ignore_index=True)\n",
    "print(len(trainingdata_6))\n",
    "traindata_day_7_1 = pd.read_csv(\"./7_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_7_4 = pd.read_csv(\"./7_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_7 = pd.concat([traindata_day_7_1, traindata_day_7_4], ignore_index=True)\n",
    "print(len(trainingdata_7))\n",
    "traindata_day_8_1 = pd.read_csv(\"./8_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_8_4 = pd.read_csv(\"./8_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_8 = pd.concat([traindata_day_8_1, traindata_day_8_4], ignore_index=True)\n",
    "print(len(trainingdata_8))\n",
    "traindata_day_9_1 = pd.read_csv(\"./9_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_9_4 = pd.read_csv(\"./9_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_9 = pd.concat([traindata_day_9_1, traindata_day_9_4], ignore_index=True)\n",
    "print(len(trainingdata_9))\n",
    "traindata_day_10_1 = pd.read_csv(\"./10_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_10_4 = pd.read_csv(\"./10_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_10 = pd.concat([traindata_day_10_1, traindata_day_10_4], ignore_index=True)\n",
    "print(len(trainingdata_10))\n",
    "traindata_day_13_1 = pd.read_csv(\"./13_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_13_4 = pd.read_csv(\"./13_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_13 = pd.concat([traindata_day_13_1, traindata_day_13_4], ignore_index=True)\n",
    "print(len(trainingdata_13))\n",
    "traindata_day_14_1 = pd.read_csv(\"./14_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_14_4 = pd.read_csv(\"./14_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_14 = pd.concat([traindata_day_14_1, traindata_day_14_4], ignore_index=True)\n",
    "print(len(trainingdata_14)\n",
    "traindata_day_15_1 = pd.read_csv(\"./15_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_15_4 = pd.read_csv(\"./15_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_15 = pd.concat([traindata_day_15_1, traindata_day_15_4], ignore_index=True)\n",
    "print(len(trainingdata_15)   \n",
    "traindata_day_16_1 = pd.read_csv(\"./16_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_16_4 = pd.read_csv(\"./16_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_16 = pd.concat([traindata_day_16_1, traindata_day_16_4], ignore_index=True)\n",
    "print(len(trainingdata_16)   \n",
    "traindata_day_17_1 = pd.read_csv(\"./17_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "traindata_day_17_4 = pd.read_csv(\"./17_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "trainingdata_17 = pd.concat([traindata_day_17_1, traindata_day_17_4], ignore_index=True)\n",
    "print(len(trainingdata_17)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Patient 10_BOS data for days 1 and 4 to create a single dataframe that will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "# Shuffle and slice the dataset.\n",
    "#train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>leftAnkle_X</th>\n",
       "      <th>leftAnkle_Y</th>\n",
       "      <th>leftAnkle_Z</th>\n",
       "      <th>leftAnkle_Magnitude</th>\n",
       "      <th>Dysk_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.445938e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.15686</td>\n",
       "      <td>9.72549</td>\n",
       "      <td>9.72675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.445938e+09</td>\n",
       "      <td>-0.15686</td>\n",
       "      <td>-0.15686</td>\n",
       "      <td>9.56863</td>\n",
       "      <td>9.5712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.445938e+09</td>\n",
       "      <td>-0.15686</td>\n",
       "      <td>-0.31372</td>\n",
       "      <td>9.56863</td>\n",
       "      <td>9.57505</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.445938e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15686</td>\n",
       "      <td>9.72549</td>\n",
       "      <td>9.72675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.445938e+09</td>\n",
       "      <td>-0</td>\n",
       "      <td>-0.15687</td>\n",
       "      <td>9.72549</td>\n",
       "      <td>9.72675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     timestamp leftAnkle_X leftAnkle_Y leftAnkle_Z  \\\n",
       "0           0  1.445938e+09           0    -0.15686     9.72549   \n",
       "1           1  1.445938e+09    -0.15686    -0.15686     9.56863   \n",
       "2           2  1.445938e+09    -0.15686    -0.31372     9.56863   \n",
       "3           3  1.445938e+09           0     0.15686     9.72549   \n",
       "4           4  1.445938e+09          -0    -0.15687     9.72549   \n",
       "\n",
       "  leftAnkle_Magnitude  Dysk_Score  \n",
       "0             9.72675           0  \n",
       "1              9.5712           0  \n",
       "2             9.57505           0  \n",
       "3             9.72675           0  \n",
       "4             9.72675           0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valdata_day_12_1 = pd.read_csv(\"./12_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "valdata_day_12_4 = pd.read_csv(\"./12_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "valdata_12 = pd.concat([valdata_day_12_1, valdata_day_12_4], ignore_index=True)\n",
    "valdata_day_19_1 = pd.read_csv(\"./19_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "valdata_day_19_4 = pd.read_csv(\"./19_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "valdata_19 = pd.concat([valdata_day_19_1, valdata_day_19_4], ignore_index=True)\n",
    "valdata_19.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Patient 17_BOS data for days 1 and 4 to create a single dataframe that will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10203483\n"
     ]
    }
   ],
   "source": [
    "valdata = pd.concat([valdata_12, valdata_19], ignore_index=True)\n",
    "X_val, Y_val = generate_data_samples(valdata, ['leftAnkle_X', 'leftAnkle_Y','leftAnkle_Z'], 'Dysk_Score', 128)\n",
    "#print(X)\n",
    "#print(Y)\n",
    "\n",
    "print(len(valdata))\n",
    "#print(X_val[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>leftAnkle_X</th>\n",
       "      <th>leftAnkle_Y</th>\n",
       "      <th>leftAnkle_Z</th>\n",
       "      <th>leftAnkle_Magnitude</th>\n",
       "      <th>Dysk_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.445245e+09</td>\n",
       "      <td>0.15686</td>\n",
       "      <td>-0.31372</td>\n",
       "      <td>9.72549</td>\n",
       "      <td>9.73181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.445245e+09</td>\n",
       "      <td>0.31372</td>\n",
       "      <td>-0.15686</td>\n",
       "      <td>9.56863</td>\n",
       "      <td>9.57505</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.445245e+09</td>\n",
       "      <td>0.15686</td>\n",
       "      <td>-0.31372</td>\n",
       "      <td>9.72549</td>\n",
       "      <td>9.73181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.445245e+09</td>\n",
       "      <td>0.15686</td>\n",
       "      <td>-0.15686</td>\n",
       "      <td>9.56863</td>\n",
       "      <td>9.5712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.445245e+09</td>\n",
       "      <td>0.15686</td>\n",
       "      <td>-0.31372</td>\n",
       "      <td>9.88235</td>\n",
       "      <td>9.88857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     timestamp leftAnkle_X leftAnkle_Y leftAnkle_Z  \\\n",
       "0           0  1.445245e+09     0.15686    -0.31372     9.72549   \n",
       "1           1  1.445245e+09     0.31372    -0.15686     9.56863   \n",
       "2           2  1.445245e+09     0.15686    -0.31372     9.72549   \n",
       "3           3  1.445245e+09     0.15686    -0.15686     9.56863   \n",
       "4           4  1.445245e+09     0.15686    -0.31372     9.88235   \n",
       "\n",
       "  leftAnkle_Magnitude  Dysk_Score  \n",
       "0             9.73181           0  \n",
       "1             9.57505           0  \n",
       "2             9.73181           0  \n",
       "3              9.5712           0  \n",
       "4             9.88857           0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata_day_11_1 = pd.read_csv(\"./11_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "testdata_day_11_4 = pd.read_csv(\"./11_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "testdata_11 = pd.concat([testdata_day_11_1, testdata_day_11_4], ignore_index=True)\n",
    "testdata_day_18_1 = pd.read_csv(\"./18_BOS_LeftLowerLimb/rawData_Day1_l.csv\")\n",
    "testdata_day_18_4 = pd.read_csv(\"./18_BOS_LeftLowerLimb/rawData_Day4_l.csv\")\n",
    "testdata_18 = pd.concat([testdata_day_18_1, testdata_day_18_4], ignore_index=True)\n",
    "testdata_18.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9957286\n"
     ]
    }
   ],
   "source": [
    "testdata = pd.concat([testdata_11, testdata_18], ignore_index=True)\n",
    "X_test, Y_test = generate_data_samples(testdata, ['leftAnkle_X', 'leftAnkle_Y','leftAnkle_Z'], 'Dysk_Score', 128)\n",
    "#print(X)\n",
    "#print(Y)\n",
    "print(len(testdata))\n",
    "#print(X_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testdata.shape\n",
    "print(X_test[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam, schedules, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From TinyML book: Very simple model to prototype\n",
    "This is a sequential model, meaning the output of each layer is passed directly into the next one.\n",
    "The first layer is Conv2D: This is a convolutional layer; it directly receives the network’s input, which is a sequence of raw accelerometer data. The input’s shape is provided in the input_shape argument. It’s set to (seq_length, 3, 1), where seq_length is the total number of accelerometer measurements that are passed in (128 by default). Each measurement is composed of three values, representing the x-, y-, and z-axes. \n",
    "The role of the convolutional layer is to take the raw data and extract some basic features that can be interpreted by subsequent layers. The arguments to the Conv2D() function determine how many features will be extracted.\n",
    "The first argument determines how many filters the layer will have. During training, each filter learns to identify a particular feature in the raw data—for example, one filter might learn to identify the signs of an upward motion. For each filter, the layer outputs a feature map that shows where the feature it has learned occurs within the input.\n",
    "The layer defined in the code has 8 filters, meaning that it will learn to recognize and output eight different types of high-level features from the input data. This is reflected in the output shape, (batch_size, 128, 3, 8), which has eight feature channels in its final dimension, one for each feature. The value in each channel indicates the degree to which a feature was present in that location of the input.\n",
    "The convolutional layers slide a window across the data and decide whether a given feature is present in that window. The second argument to Conv2D() is where we provide the dimensions of this window. In our case, it’s (4, 3). This means that the features for which our filters are hunting span four consecutive accelerometer measurements and all three axes. Because the window spans four measurements, each filter analyzes a small snapshot of time, meaning it can generate features that represent a change in acceleration over time. \n",
    "When used for classification, the goal of a CNN is to transform a big, complex input tensor into a small, simple output. This is what the MaxPool2D layer does by congesting the output of the first convolutional layer into a concentrated, high-level representation of the most relevant features it contains. Even though the original input has 3 accelerometer axes for each measurement, a combination of Conv2D and MaxPool2D merges these together into a single value.\n",
    "The Dropout layer randomly sets some of a tensor’s values to zero during training. In this case, by calling Dropout(0.1), we set 10% of the values to zero, entirely obliterating that data. Dropout is a regularization technique and process of improving machine learning models so that they are less likely to overfit their training data. By randomly removing some data between one layer and the next, we force the neural network to learn how to cope with unexpected noise and variation.\n",
    "There can be more Conv2D and MaxPooling layers; let us strat with 2.\n",
    "The Flatten layer is used to transform a multidimensional tensor into one with a single dimension. In this case, our (14, 1, 16) tensor is squished down into a single dimension with shape (224).\n",
    "It’s then fed into a Dense layer with 16 neurons, which considersg all data at once, learning the meanings of various combinations of inputs. The output of this Dense layer will be a set of 16 values representing the content of the original input in a highly compressed form.\n",
    "The final task is to shrink these 16 values down into 5 classes. To do this, we first add some more dropout and then a final Dense layer, which has four neurons; one representing each class of gesture. Each of them is connected to all 16 of the outputs from the previous layer. During training, each neuron will learn the combination of previous-layer activations that correspond to the gesture it represents. \n",
    "The layer is configured with a \"softmax\" activation function, which results in the layer’s output being a set of probabilities that sum to 1. This output is what we see in the model’s output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(# input_shape=(batch, 128, 3)\n",
    "        8, (4, 3),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        input_shape=(128, 3, 1)), #output_shape=(batch, 128, 3, 8)\n",
    "    tf.keras.layers.MaxPool2D((3, 3)),  # (batch, 42, 1, 8)\n",
    "    tf.keras.layers.Dropout(0.1),  # (batch, 42, 1, 8)\n",
    "    tf.keras.layers.Conv2D(16, (4, 1), padding=\"same\", activation=\"relu\", input_shape=(42, 1, 16)),\n",
    "    tf.keras.layers.MaxPool2D((3, 1), padding=\"same\"),  # (batch, 14, 1, 16)\n",
    "    tf.keras.layers.Dropout(0.1),  # (batch, 14, 1, 16)\n",
    "    tf.keras.layers.Flatten(),  # (batch, 224)\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),  # (batch, 16)\n",
    "    tf.keras.layers.Dropout(0.1),  # (batch, 16)\n",
    "    tf.keras.layers.Dense(5, activation=\"softmax\")  # (batch, 4)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a classifier with multiple classes. Thus, the fitting includes class weights. In most cases, classes are not perfectly balanced, so this addition takes this into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_classes = np.unique(Y_train)\n",
    "print(Y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1., 1: 48., 2: 137., 3: 543., 4: 7780.,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(Y_train))\n",
    "print(X_train[0:3])\n",
    "print(Y_train[0:3])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./3_BOS_LeftLowerLimb/rawData_Day1_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0899 - accuracy: 0.9759\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0896 - accuracy: 0.9759\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0891 - accuracy: 0.9759\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0884 - accuracy: 0.9759: 2s - loss: 0.0887 - ac\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0877 - accuracy: 0.9759\n",
      "./3_BOS_LeftLowerLimb/rawData_Day4_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0877 - accuracy: 0.9759\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0874 - accuracy: 0.9759\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0876 - accuracy: 0.9759\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0866 - accuracy: 0.9759\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0863 - accuracy: 0.9759\n",
      "./4_BOS_LeftLowerLimb/rawData_Day1_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0858 - accuracy: 0.9759\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 5s 5ms/step - loss: 0.0860 - accuracy: 0.9759\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0860 - accuracy: 0.9759\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0858 - accuracy: 0.9759\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0855 - accuracy: 0.9759\n",
      "./4_BOS_LeftLowerLimb/rawData_Day4_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0851 - accuracy: 0.9759\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0858 - accuracy: 0.9759\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0856 - accuracy: 0.9759\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0864 - accuracy: 0.9759\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0849 - accuracy: 0.9759\n",
      "./5_BOS_LeftLowerLimb/rawData_Day1_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0856 - accuracy: 0.9759\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0853 - accuracy: 0.9759\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0852 - accuracy: 0.9759\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0854 - accuracy: 0.9759\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0847 - accuracy: 0.9759\n",
      "./5_BOS_LeftLowerLimb/rawData_Day4_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0849 - accuracy: 0.9759\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0849 - accuracy: 0.9759\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0853 - accuracy: 0.9759\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0842 - accuracy: 0.9759\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0848 - accuracy: 0.9759\n",
      "./6_BOS_LeftLowerLimb/rawData_Day1_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0839 - accuracy: 0.9759\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0838 - accuracy: 0.9759\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0843 - accuracy: 0.9759\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0844 - accuracy: 0.9759\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0838 - accuracy: 0.9759\n",
      "./6_BOS_LeftLowerLimb/rawData_Day4_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0835 - accuracy: 0.9759\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0833 - accuracy: 0.9759\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0838 - accuracy: 0.9759\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0839 - accuracy: 0.9759\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0836 - accuracy: 0.9759\n",
      "./7_BOS_LeftLowerLimb/rawData_Day1_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0824 - accuracy: 0.9760\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0833 - accuracy: 0.9758\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 5s 5ms/step - loss: 0.0831 - accuracy: 0.9759\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 5s 5ms/step - loss: 0.0828 - accuracy: 0.9759\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0829 - accuracy: 0.9759\n",
      "./7_BOS_LeftLowerLimb/rawData_Day4_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0825 - accuracy: 0.9760\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0825 - accuracy: 0.9759\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 5s 5ms/step - loss: 0.0826 - accuracy: 0.9760\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0832 - accuracy: 0.9760\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0832 - accuracy: 0.9760\n",
      "./8_BOS_LeftLowerLimb/rawData_Day1_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0824 - accuracy: 0.9760\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0831 - accuracy: 0.9759\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0827 - accuracy: 0.9759\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0823 - accuracy: 0.9759\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 5s 5ms/step - loss: 0.0827 - accuracy: 0.9761\n",
      "./8_BOS_LeftLowerLimb/rawData_Day4_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0827 - accuracy: 0.9759\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0815 - accuracy: 0.9762\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0815 - accuracy: 0.9763\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0821 - accuracy: 0.9761\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0823 - accuracy: 0.9758\n",
      "./9_BOS_LeftLowerLimb/rawData_Day1_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0816 - accuracy: 0.9761\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0823 - accuracy: 0.9760\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0827 - accuracy: 0.9758\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0824 - accuracy: 0.9759\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0830 - accuracy: 0.9760\n",
      "./9_BOS_LeftLowerLimb/rawData_Day4_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0836 - accuracy: 0.9760\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0826 - accuracy: 0.9760\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0816 - accuracy: 0.9761\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0814 - accuracy: 0.9763\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0821 - accuracy: 0.9762\n",
      "./10_BOS_LeftLowerLimb/rawData_Day1_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0815 - accuracy: 0.9762\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0819 - accuracy: 0.9763\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0809 - accuracy: 0.9763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0821 - accuracy: 0.9760\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0816 - accuracy: 0.9762\n",
      "./10_BOS_LeftLowerLimb/rawData_Day4_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0814 - accuracy: 0.9763\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 5s 5ms/step - loss: 0.0814 - accuracy: 0.9761\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 5s 5ms/step - loss: 0.0815 - accuracy: 0.9762\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0819 - accuracy: 0.9760\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0811 - accuracy: 0.9761\n",
      "./13_BOS_LeftLowerLimb/rawData_Day1_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0812 - accuracy: 0.9759\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0813 - accuracy: 0.9761\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 5s 5ms/step - loss: 0.0814 - accuracy: 0.9760\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0809 - accuracy: 0.9762\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0817 - accuracy: 0.9764\n",
      "./13_BOS_LeftLowerLimb/rawData_Day4_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0816 - accuracy: 0.9760\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0814 - accuracy: 0.9759\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0818 - accuracy: 0.9760\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0813 - accuracy: 0.9762\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0820 - accuracy: 0.9762\n",
      "./14_BOS_LeftLowerLimb/rawData_Day1_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0813 - accuracy: 0.9763\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0824 - accuracy: 0.9762\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0817 - accuracy: 0.9761\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0820 - accuracy: 0.9761\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0813 - accuracy: 0.9761\n",
      "./14_BOS_LeftLowerLimb/rawData_Day4_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 5ms/step - loss: 0.0808 - accuracy: 0.9761\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0813 - accuracy: 0.9761\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0805 - accuracy: 0.9760\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0805 - accuracy: 0.9767\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0804 - accuracy: 0.9763\n",
      "./15_BOS_LeftLowerLimb/rawData_Day1_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0810 - accuracy: 0.9763\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0816 - accuracy: 0.9762\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0819 - accuracy: 0.9761\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0811 - accuracy: 0.9761\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0805 - accuracy: 0.9763\n",
      "./15_BOS_LeftLowerLimb/rawData_Day4_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0826 - accuracy: 0.9760\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0805 - accuracy: 0.9763\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0807 - accuracy: 0.9763\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0801 - accuracy: 0.9765: 0s - l\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0809 - accuracy: 0.9763\n",
      "./16_BOS_LeftLowerLimb/rawData_Day1_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0813 - accuracy: 0.9762\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0807 - accuracy: 0.9763\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0807 - accuracy: 0.9765\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0796 - accuracy: 0.9765\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0804 - accuracy: 0.9761\n",
      "./16_BOS_LeftLowerLimb/rawData_Day4_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0802 - accuracy: 0.9765\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0793 - accuracy: 0.9765\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0799 - accuracy: 0.9764\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0824 - accuracy: 0.9761\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0805 - accuracy: 0.9762\n",
      "./17_BOS_LeftLowerLimb/rawData_Day1_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0805 - accuracy: 0.9763\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0802 - accuracy: 0.9761\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0806 - accuracy: 0.9764\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0805 - accuracy: 0.9762\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0799 - accuracy: 0.9763\n",
      "./17_BOS_LeftLowerLimb/rawData_Day4_l.csv\n",
      "38573\n",
      "38573\n",
      "Epoch 1/5\n",
      "1206/1206 [==============================] - 5s 4ms/step - loss: 0.0802 - accuracy: 0.9764\n",
      "Epoch 2/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0807 - accuracy: 0.9761\n",
      "Epoch 3/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0806 - accuracy: 0.9765\n",
      "Epoch 4/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0808 - accuracy: 0.9762\n",
      "Epoch 5/5\n",
      "1206/1206 [==============================] - 6s 5ms/step - loss: 0.0799 - accuracy: 0.9763\n"
     ]
    }
   ],
   "source": [
    "# fit - train the model\n",
    "for data_file in file_names:\n",
    "    training_dataframe = pd.read_csv(data_file)\n",
    "    print(data_file)\n",
    "    X_train, Y_train = generate_data_samples(trainingdata, ['leftAnkle_X', 'leftAnkle_Y','leftAnkle_Z'], 'Dysk_Score', 128)\n",
    "    print(len(X_train))\n",
    "    print(len(Y_train))\n",
    "    model.fit(x=X_train, y=Y_train, epochs=5, verbose=1, class_weight=class_weight) \n",
    "        #batch_size=128,\n",
    "        #validation_data=(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2492/2492 [==============================] - 7s 2ms/step - loss: nan - accuracy: 0.9868\n",
      "[1. 0. 0. 0. 0.]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "#evaluate val_dataset before testing on test data. \n",
    "model.evaluate(X_val, Y_val)\n",
    "classifications = model.predict(X_val)\n",
    "print(classifications[0])\n",
    "print(Y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2431/2431 [==============================] - 5s 2ms/step - loss: nan - accuracy: 0.9700\n",
      "tf.Tensor(\n",
      "[[75459     1     0     0     0]\n",
      " [ 1613     0     0     0     0]\n",
      " [  565     0     0     0     0]\n",
      " [  143     0     0     0     0]\n",
      " [   10     0     0     0     0]], shape=(5, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)\n",
    "pred = np.argmax(model.predict(X_test), axis=1)\n",
    "confusion = tf.math.confusion_matrix(labels=tf.constant(Y_test),\n",
    "                                       predictions=tf.constant(pred),\n",
    "                                       num_classes=5)\n",
    "print(confusion)\n",
    "\n",
    "\n",
    "#print(\"Loss {}, Accuracy {}\".format(loss, acc))\n",
    "# Convert the model to the TensorFlow Lite format without quantization\n",
    "#converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "#tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "#converter = tf.lite.TFLiteConverter.from_keras_model('CNNmodel.h5')\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# These options are for converter optimizaitons\n",
    "# Try the converter without them and explore model size and accuracy\n",
    "# Then use them and reconvert the model and explore model\n",
    "# size an accuracy at that point.\n",
    "\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]    # Uncomment this line for Model 2 and Model 3\n",
    "\n",
    "#def representative_data_gen():                          # Uncomment the following 5 lines for Model 3\n",
    "#     for input_value, _ in X_test.take(100):\n",
    "#         yield [input_value]\n",
    "#converter.representative_dataset = representative_data_gen\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "#tflite_models_file = pathlib.Path(\"./tmp/model1.tflite\")\n",
    "tflite_model_file = pathlib.Path(\"./tmp/model1.tflite\")     # Change the filename here for Model2 and Model3!\n",
    "tflite_model_file.write_bytes(tflite_model)\n",
    "# Without any optimizations I got\n",
    "# 20236  (model1.tflite) - the model that was created with 10 epochs\n",
    "# With the .optimizations property set I got\n",
    "# 9728 (model2.tflite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell each time to test your model's accuracy (make sure to change the filename). \n",
    "#Tqdm is a Python library used to display smart progress bars that show the progress of Python code execution.\n",
    "from tqdm import tqdm\n",
    "# Load TFLite model and allocate tensors.\n",
    "tflite_model_file = './tmp/model1.tflite'                 # Change the filename here for Model 2 and 3 with optimisations\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_file)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "print(input_index)\n",
    "print(output_index)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(0,100)):\n",
    "    data = X_test[i]\n",
    "    data = data.reshape(1, data.shape[0], data.shape[1], data.shape[2]) \n",
    "    #print(data.shape)\n",
    "    label = Y_test[i]\n",
    "    interpreter.set_tensor(input_index, data)\n",
    "    interpreter.invoke()\n",
    "    predictions.append(interpreter.get_tensor(output_index))\n",
    "    \n",
    "#    Y_test_labels.append(label.numpy()[0])\n",
    "#    X_test.append(X_test)\n",
    "\n",
    "# For model 1, \n",
    "# For model 2, \n",
    "# For model 3, \n",
    "# Note: since the it/s will depend on the computer on which the model instance is running -- it would vary a bit.\n",
    "\n",
    "score = 0\n",
    "for item in range(0,100):\n",
    "  prediction=np.argmax(predictions[item])\n",
    "  label = Y_test[item]\n",
    "  if prediction==label:\n",
    "    score=score+1\n",
    "\n",
    "print(\"Out of 100 predictions I got \" + str(score) + \" correct\")\n",
    "\n",
    "# Model 1 - 100 Correct\n",
    "# Model 2 - 99 Correct\n",
    "# Model 3 - 99 Correct\n",
    "# Note: training starts from a random intialization the result could be off by 1 or 2 correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
